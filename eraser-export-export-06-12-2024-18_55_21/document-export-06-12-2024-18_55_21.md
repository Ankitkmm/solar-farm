# Software Engineering Challenge - System Integration





Problem Statement
Design a scalable system that integrates real-time data collection, model inference, and
visualization for a solar farm monitoring system.



1.  Architecture Design


### **Parameters for the Solar Farm Monitoring System**
#### **1. Data Collection**
- **Parameters to Monitor:**
    - Solar panel **temperature** (°C).
    - **Irradiance** (W/m²) – sunlight intensity.
    - **Power Output** (W) – energy generated.
    - **Tilt Angle** (degrees) – orientation of the panel.
We have a bunch of solar panels out in the field, each sending us data—things like temperature, how much sunlight they’re getting, and the power they’re producing. We need a system that takes all that data in real-time, checks that it’s good (no crazy numbers), figures out what it means (are we running smoothly, is something off?), and then shows us the results in a simple dashboard so we can act on it.



**The Gateway:**  All the data from the solar panels shows up there first. The gateway makes sure we don’t let in junk data or let the system get overwhelmed.

**Quality Check & Routing:** Right after the gateway, we have a service that acts like a quality control inspector. It checks if the data makes sense (no negative temperatures in a sunny field, for example). 

**Data Pipeline (Kafka):**  Once the data is on it, it’s always moving forward, and it’s easy to add more panels or handle more data without things clogging up. Kafka just keeps everything flowing—fast and reliably.

**Storage & Records:** We keep the time-based data (stuff that changes second by second, minute by minute) in one special database designed for quickly looking up historical trends.

**The Dashboard:**  It’s like a control room screen showing you real-time performance, how things changed over time, and where any problems are cropping up. 

#### 1. Ingestion API (Data Collection Service)
**Endpoint:** `POST /ingest`
**Role:** Receives data from the API Gateway after basic routing.

**Validation:**

- Check required fields (e.g., panelId, timestamp, powerOutput).
- Validate ranges (e.g., temperature should be within a plausible range).
**Response:**

- `200 OK`  with a message confirming data acceptance or
- `400 Bad Request`  if validation fails.
**Data Flow Action:**
If valid, the Data Collection Service writes the reading into a Kafka topic 



#### 2. Processing Service APIs
**Model Inference Endpoint (Internal):** `POST /process`
**Role:** Consumes messages from Kafka internally, applies predictive models, and may store enriched data.

**Processing Steps:**

- Aggregate data if needed (e.g., compute hourly averages).
- Run predictive models for anomaly detection or performance forecasts.
- If anomalies are detected, store them in PostgreSQL.
- Store raw and processed metrics in InfluxDB for time-series queries.
**Response:**

- `200 OK`  once the data is processed and written to the databases.
- `500 Internal Server Error`  if any processing step fails (logged for debugging).
#### 3. Visualization Backend API
This API enables the front-end React Dashboard to query processed data. It’s typically a RESTful set of endpoints that query InfluxDB for time-series metrics and PostgreSQL for alerts and metadata.

**Endpoints Examples:**

- **GET /api/metrics/realtime?panelId=...**
**Role:** Retrieve recent measurements (e.g., last 5 minutes of powerOutput data) from InfluxDB.
- **GET /api/alerts?farmId=...**
**Role:** List recent anomalies or alerts stored in PostgreSQL.
- **GET /api/history?panelId=...&startTime=...&endTime=...**
**Role:** Fetch historical data (e.g., one week’s worth of hourly averages from InfluxDB) for analysis.
**Response:** JSON array of aggregated time-series data points.
**Data Flow Action:** The Visualization Backend queries InfluxDB for real-time metrics and PostgreSQL for alerts/metadata, then returns results to the React dashboard.

- Document scaling strategy 

As system load grows, we’ll scale horizontally. Add more API Gateway instances and Data Collection Service nodes behind a load balancer. Increase Kafka partitions and brokers so multiple Processing Service instances can handle more data in parallel. For databases, add read replicas to PostgreSQL and leverage InfluxDB’s clustering or sharding for time-series scaling. Use autoscaling triggers (e.g., CPU, request rates, or Kafka consumer lag) to spin up new service instances. Implement caching (Redis) to reduce database load, and serve the dashboard via a CDN for global performance. Continuous monitoring (Prometheus, Grafana) and logging (ELK) guide when and how to scale further.







